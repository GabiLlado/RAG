{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22241052",
   "metadata": {},
   "source": [
    "### Modelo OpenAI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4193d22",
   "metadata": {},
   "source": [
    "En este fichero aprendo a gestionar Langchain usando como modelo ChatGPT mediante su API. Trataré de conectar con Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f106e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import config\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_pinecone import PineconeVectorStore, PineconeEmbeddings\n",
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "520ae0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = config.OPENAI_API_KEY\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36a47cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! Sí, estoy funcionando. ¿En qué puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(\"Hola, ¿puedes confirmarme que estás funcionando?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0244370",
   "metadata": {},
   "source": [
    "Ahora voy a usar el modelo tratando de recuperar los archivos almacenados en Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6521a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", api_key=config.OPENAI_API_KEY)\n",
    "\n",
    "# Pinecone\n",
    "pc = Pinecone(api_key=config.PINECONE_API_KEY)\n",
    "index = pc.Index(config.INDEX_PINECONE)\n",
    "\n",
    "embedding = PineconeEmbeddings(\n",
    "    model=config.embedding_model,\n",
    "    api_key=config.PINECONE_API_KEY\n",
    ")\n",
    "\n",
    "vector_store = PineconeVectorStore(embedding=embedding, index=index, text_key=\"body\")\n",
    "\n",
    "# Recuperar documentos de Pinecone para usarlos como contexto\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900bdb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Putin conocía a Roman Starovoit, ya que él era un exministro de Transporte que fue despedido por el presidente ruso antes de su muerte, la cual ha sido considerada sospechosa.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 3713, 'total_tokens': 3755, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C8XgS9TccBY1jQWFMwAlyoPx0H7uW', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--79a372e4-8898-4a22-8f75-35a418f70219-0' usage_metadata={'input_tokens': 3713, 'output_tokens': 42, 'total_tokens': 3755, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Generar respuesta usando el modelo y los documentos recuperados\n",
    "\n",
    "# Defino un prompt con el contexto, poniendo al bot en situación y haciendo la pregunta. Esto es una plantilla.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Responde solo con el contexto. Si falta info, dilo.\"),\n",
    "    (\"user\", \"Pregunta: {question}\\n\\nContexto:\\n{context}\")\n",
    "])\n",
    "\n",
    "# Formateo los documentos para el prompt. De momento no uso los metadatos de las noticias\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"• {d.page_content}\" for d in docs)\n",
    "\n",
    "# Construyo el pipeline para el RAG\n",
    "rag = (\n",
    "    RunnableParallel(\n",
    "        context=retriever | RunnableLambda(format_docs),\n",
    "        question=lambda x: x\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "# Hago la pregunta y obtengo la respuesta\n",
    "query = \"Putin conoce Roman?\"\n",
    "\n",
    "result = rag.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2cc018",
   "metadata": {},
   "source": [
    "Cosas que quiero mejorar:\n",
    "1. Tener conversación e historial de chat.\n",
    "2. Gestionar los idiomas, hacer algo de prompt engineering.\n",
    "3. Al recuperar documentos me gustaría poner en práctica añadir docs randoms si mejoran 35% la respuesta.\n",
    "4. Aplicar cosillas de Advanced y Modular RAG."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
